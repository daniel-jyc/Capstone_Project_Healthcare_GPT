{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95deea83-fbdd-4c9a-8a5c-9849ba30d13d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MedAlpaca Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb1578-c2d3-4a4d-a3a4-527fd87af7b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initiate config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2f3a0f-04a3-42b3-b0c1-5b5498c4adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd medalpaca-7b-capstone/\n",
    "\n",
    "# nvidia-smi\n",
    "# # sudo kill -9 29431 (29431 is PID in nvidia-smi)\n",
    "# df -h\n",
    "# du -h --max-depth=1 /home/jupyter | sort -rh\n",
    "# rm -rf /home/jupyter/.cache/*\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f14201db-d1e0-4038-bfe3-28df7ef57ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Settings\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Setting for A100\n",
    "MICRO_BATCH_SIZE = 4\n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 2 # paper uses 3\n",
    "LEARNING_RATE = 2e-5  # from the original paper\n",
    "CUTOFF_LEN = 512\n",
    "TRAIN_STEPS = 300 # adjust for training time and efficiency\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f995347-c7cf-4ef3-9685-b3c113266ee4",
   "metadata": {},
   "source": [
    "## Loading MedAlpaca 7B Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c484e5-7332-4880-b651-1da0e0009a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479ae670482e46448fb7e988e15ee686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'LLaMATokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LLaMATokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"medalpaca/medalpaca-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = LLaMATokenizer.from_pretrained(\n",
    "    \"medalpaca/medalpaca-7b\", add_eos_token=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8878b9-2fa0-4ec1-9d71-cf8956298f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essential methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97136e47-5e4c-4111-83eb-3c534c896628",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 512\n",
    "\n",
    "def generate_prompt(data_point, cutoff_len):\n",
    "    available_space = cutoff_len - len(\"### Instruction:\\n\") - len(data_point[\"instruction\"]) - len(\"\\n### Input:\\n\") - len(\"\\n### Response:\\n\")\n",
    "    \n",
    "    # Truncate the input text to fit within the available space\n",
    "    truncated_input = data_point[\"input\"][:available_space]\n",
    "    \n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True, # truncation was set to True, making the response not shown properly when the input is too long\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point, 512)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514b77e-75fc-47aa-ba63-1dd996aec024",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af595185-5761-475f-a0c6-ad41385d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ecdbc2c-0039-4c84-854f-850c1b542bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jupyter/.cache/huggingface/datasets/json/default-6606eaf40512cd75/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8291a50200e746b2bc86515fbfbc8e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data = load_dataset(\"json\", data_files=\"CT_InstructionTuning.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "102fdfaa-266e-42e9-9174-e2f29caf1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate LoraConfig\n",
    "from peft import LoraConfig, get_peft_model, set_peft_model_state_dict, get_peft_model_state_dict\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fff53e0e-7545-49b1-a491-c9ff28d01151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/jupyter/.cache/huggingface/datasets/json/default-6606eaf40512cd75/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b170efa1078d55ae.arrow and /home/jupyter/.cache/huggingface/datasets/json/default-6606eaf40512cd75/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-768a7e750b79d253.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create validation dataset \n",
    "train_val = data[\"train\"].train_test_split(test_size=300, shuffle=True, seed=42)\n",
    "\n",
    "train_data = (train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt))\n",
    "val_data = (train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12dcf55d-179d-4e0a-b20a-a54aff8602e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "512\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Generate impression based on clinical information and findings.\n",
      "### Input:\n",
      "Status post type a aortic dissection repair history: status post repair. Interval development of large left pleural effusion. Moderate to severe upper lobe predominant emphysema. Right lung base subpleural bullae. Interval repair of ascending aortic aneurysm. Interval aneurysmal dilation of the descending aorta. At the level of the carina the descending aorta measures 4.9 x 5.7 cm , previously 3.5 x 3.9 cm. At the level of the diaphragmatic hiatus the descending aorta measures 4.3 x 4.7 cm , previously 3.7 x 4.1 cm. There is slow enhancement of the false lumen indicating a fenestration of the dissection flap. Right carotid stent in place. Dilated pulmonary artery compatible with pulmonary hypertension. Subcarinal lymph node measuring 2.2 x 1.6 cm , stable from prior study. Moderate. No significant abnormality noted. Enlarged hepatic veins noted. Atrophic spleen with cortical irregularity. Nonspecific bilateral adrenal thickening. There is fenestration of the dissection flap with contrast seen in both the false and true lumen of the abdominal aortic aneurysm. Dissection flap extends into the celiac artery and terminates at the level of the SMA. Celiac artery is patent. SMA is supplied by true lumen and is patent. Renal arteries are patent. The IMA is patent. Colonic diverticulosis without evidence of diverticulitis. Mild vertebral height loss of L4 vertebrae.\n",
      "### Response:\n",
      "Significant interval aneurysmal dilatation of the descending thoracic aorta with measurements given above. Interval repair of ascending aortic dissection. Interval development of large left pleural effusion. Findings discussed with Dr. 40 AM.\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_id = 0\n",
    "for i in range(7700):\n",
    "    length = len(train_data[i]['input_ids'])\n",
    "    if max_len < length:\n",
    "        max_len = length\n",
    "        max_id = i\n",
    "print(max_id)\n",
    "print(max_len)\n",
    "\n",
    "# print('\\n',train_data[max_id]['input'])\n",
    "# print('\\n',train_data[max_id]['output'])\n",
    "\n",
    "print(generate_prompt(train_data[max_id], 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c10e29-b746-456d-a7c7-5de82d032c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ebc96a7-46b3-48bf-8994-ce466dc6c55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 1:08:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.634600</td>\n",
       "      <td>2.622983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.328400</td>\n",
       "      <td>2.276741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.966000</td>\n",
       "      <td>1.936463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.795400</td>\n",
       "      <td>1.784597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.806100</td>\n",
       "      <td>1.741016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.715800</td>\n",
       "      <td>1.724318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=2.100595785776774, metrics={'train_runtime': 4153.9806, 'train_samples_per_second': 4.622, 'train_steps_per_second': 0.072, 'total_flos': 3.609343397666488e+17, 'train_loss': 2.100595785776774, 'epoch': 2.5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate Training with Lora Config\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=100,\n",
    "        max_steps = TRAIN_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        output_dir=\"medalpaca_results\",\n",
    "        save_total_limit=3,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))()\n",
    "\n",
    "set_peft_model_state_dict(model, state_dict)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c3dd9f-2c50-4376-a57d-881756cdf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save_pretrained(\"medalpaca_7b_capstone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb780095-cbd8-44d5-8f55-accc2d17d364",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save model to huggingface page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723a8fe3-fb99-430e-a4ae-2011da213f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029fcccc36374258a6a4cfa3c170cc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103607da-b8cd-474a-a511-091b2b6dfb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6815eedf4af549679d26daa9b8295452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab157415e9a34785af9537f4cb3c4acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Danieljyc/medalpaca_7b_capstone/commit/d8cde8c50ad1a300c609cb9d67981af568826cba', commit_message='Upload model', commit_description='', oid='d8cde8c50ad1a300c609cb9d67981af568826cba', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Danieljyc/medalpaca_7b_capstone\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca84c92-a7b1-4647-ac52-d1c3c284ad7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8cd0ec5-55d0-479a-9eac-69c6923eeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74eddac5-aa37-4324-9c03-44c5addc1a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'LLaMATokenizer'.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51288099a9294db6970bedbad1d6e824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
    "tokenizer = LLaMATokenizer.from_pretrained(\"medalpaca/medalpaca-7b\")\n",
    "\n",
    "model = LLaMAForCausalLM.from_pretrained(\n",
    "    \"medalpaca/medalpaca-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"Danieljyc/medalpaca_7b_capstone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92d4d2fa-99ea-4dba-8125-6f8889f50da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config generation settings\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46766c40-512e-4b6a-ba5a-9cd8a95433de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating... Generated 50 impressions.\n",
      "Generating... Generated 100 impressions.\n",
      "Generating... Generated 150 impressions.\n",
      "Generating... Generated 200 impressions.\n",
      "Generating... Generated 250 impressions.\n",
      "Generating... Generated 300 impressions.\n"
     ]
    }
   ],
   "source": [
    "generated_impressions = []\n",
    "original_impressions = []\n",
    "generated_counts = 0\n",
    "\n",
    "for data_point in val_data:\n",
    "    inputs = generate_and_tokenize_prompt(data_point)\n",
    "    \n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0)\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids.to('cuda'),\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    generated_counts+=1\n",
    "    \n",
    "    for s in generation_output.sequences:\n",
    "        impression_tokens = tokenizer.decode(s)\n",
    "    \n",
    "    if generated_counts % 50 == 0:\n",
    "        print(f'Generating... Generated {generated_counts} impressions.')\n",
    "    \n",
    "    generated_impression = impression_tokens.split(\"### Response:\")[-1].split(\"</s>\")[0].strip()\n",
    "    \n",
    "    generated_impressions.append(generated_impression)\n",
    "    original_impressions.append(data_point['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f5a47-c563-446c-b134-7b7aa822bbb8",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dacc167-d634-4241-abcd-7692e447d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7303aefd-2350-476b-a712-764af66da13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a786984c2c014b6bbb9d33275987afe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge_result = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8135beed-9c8a-4e14-9e03-de2afb671111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.8718436880237352, recall=0.9030677166912301, fmeasure=0.860219247125902), mid=Score(precision=0.903166478217011, recall=0.9241705214603966, fmeasure=0.8901430408063447), high=Score(precision=0.9322995823400029, recall=0.944964532202922, fmeasure=0.9192132211433116)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.859028023338243, recall=0.8585548059326111, fmeasure=0.8439612262686172), mid=Score(precision=0.8913050020031232, recall=0.8890489431961077, fmeasure=0.8769190538164033), high=Score(precision=0.9241021865237158, recall=0.9168513901057519, fmeasure=0.9091053080124466)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.8649746303132518, recall=0.876137503508571, fmeasure=0.8486046824932391), mid=Score(precision=0.8975516146484162, recall=0.9029343766983999, fmeasure=0.88307768766514), high=Score(precision=0.9288085755338452, recall=0.9287765408860261, fmeasure=0.9150868265775309)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.863418452885316, recall=0.8779655033079121, fmeasure=0.8482777766531502), mid=Score(precision=0.8969657282915033, recall=0.9040172880218622, fmeasure=0.8820308715800279), high=Score(precision=0.9254298762242451, recall=0.9279465168204454, fmeasure=0.9119442070053824))}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_result.compute(predictions= generated_impressions, references= original_impressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fabdd1-f418-4879-93c2-2adab8719518",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c68630e-084a-4909-8e85-5766c1117e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdbda2c2c5b46b5ba86de6b0d166063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95570374429346e3a83b0f1b3cb37211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_result = load_metric('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7144877c-953d-43df-a4ba-dac27cb6c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "split_generated_impressions = [re.split(r'\\s+|-', text) for text in generated_impressions]\n",
    "list_of_split_generated_impressions = [list(inner_array) for inner_array in split_generated_impressions]\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "tokenized_sentences = [sent_tokenize(paragraph) for paragraph in original_impressions]\n",
    "split_original_impressions = [[word_tokenize(sentence) for sentence in paragraph] for paragraph in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b7fed80-2a08-4af4-83c8-d610d86ab0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3100091749178359,\n",
       " 'precisions': [0.38318009734991887,\n",
       "  0.34040681693238045,\n",
       "  0.28988261598658466,\n",
       "  0.24427263941788416],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 7.1252408477842,\n",
       " 'translation_length': 18490,\n",
       " 'reference_length': 2595}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_result.compute(predictions= list_of_split_generated_impressions, references= split_original_impressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b622b-b6fa-46d6-a264-f5ccadfa7a86",
   "metadata": {},
   "source": [
    "## other codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c240ebf-aeb5-4c90-88bf-e8e10d43ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Generate impression based on clinical information and findings.\n",
      "### Input:\n",
      "persistent headache and neck pain, dizziness. History of gunshot wound with residual shrapnel to t4/t5. CervicalThe cervical vertebral bodies are appropriate height. Alignment is maintained. No fractures are identified in the cervical spine. No destructive osseous lesions are identified in the cervical spine. Nonspecific lytic lesion involving the right upper aspect of the C4 vertebral body as well as the posterior aspect of the T1 vertebral body. There is also a sclerotic focus involving the T2 vertebral body. Small osseous protuberance along the left inferior facet of C6 may represent a tiny osteochondroma. C2-3: No significant compromise to the spinal canal or neural foramina. No significant compromise to the spinal canal or neural foramina. Thoracic There is irregularity involving the posterior inferior aspect of the T4 vertebral body likely related to remote fracture. There is also sclerosis and osteophyte projecting from the posterior superior aspect of the T5 vertebral body. These findings are most compatible with sequela of remote trauma. Metallic density is noted involving the anterior spinal canal at the T4-T5 level compatible with shrapnel. Similar densities also noted involving the right posterior thoracic wall, right pedicle or transverse process junction at the T3 vertebral level, pleural surface as well as punctate focus in the lung parenchyma. Deformity seen involving the right posterior third and fourth ribs as well as right transverse process of T4, findings most compatible with healed fractures. There is mild right T4-T5 neural foramina stenosis related to facet osteophyte. Remainder of the thoracic spine demonstrates normal vertebral body heights and alignment and without spinal canal or neural foramina stenosis. Moderate degree of pulmonary emphysema noted.\n",
      "### Response:\n",
      "There is no evidence for acute abnormality in the cervical spine. The presence of small osteochondromas and osteophytes is not indicative of any significant pathology. The presence of metallic artifacts within the spinal canal is most likely due to prior injury.</s>\n",
      "\n",
      " Chronic fracture deformity involving the posterior inferior corner of the T4 vertebral body on the left. Sclerosis and osteophyte formation at the T4-T5 level also compatible with sequela of prior trauma. Deformity of right T4 transverse process and the right fourth and fifth ribs also compatible with prior trauma. Dorsally projecting osteophyte at T4-T5 narrows the spinal canal. There is also an adjacent metallic fragment in the ventral spinal canal at the T4-T5 compatible with shrapnel. Subcentimeter areas of lucency and sclerosis such as at C4, T1, and T2 may be benign, but remain nonspecific by imaging. Follow-up can be considered as clinically indicated.\n"
     ]
    }
   ],
   "source": [
    "id = 254\n",
    "\n",
    "inputs = generate_and_tokenize_prompt(val_data[id])\n",
    "input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0)\n",
    "generation_output = model.generate(\n",
    "        input_ids=input_ids.to('cuda'),\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "for s in generation_output.sequences:\n",
    "    print(tokenizer.decode(s))\n",
    "    \n",
    "print('\\n',val_data[id]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b867df94-c2fc-4585-a244-dfbb1d40b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Generate impression based on clinical information and findings.\n",
      "### Input:\n",
      " There is transitional lumbosacral anatomy, with partial lumbarization of the S1 vertebra. For the purposes of this exam, the last fully formed disc is at S1-S2 with last rib-bearing vertebra designated as T12. Postoperative changes are seen from posterior surgical fusion of L5 and S1, with bilateral pedicle screws and interconnecting rods. There is no evidence of instrumentation complication. There is no cortical breach. There is evidence of left facetectomy at the L5-S1 level. Interbody spacer is present with associated bone graft material. Streak artifact from the instrumentation limits evaluation of surrounding structures. There are a few foci of air scattered in the subcutaneous soft tissues as well as a few in the left ventral epidural space, likely postoperative. Postoperative fluid is seen along the surgical tracts in the subcutaneous fat, greater on the left side. The scout lateral view and the sagittal reformatted images demonstrate the lumbar spine to be in normal alignment, with exaggeration of the normal lumbar lordosis. There is trace grade 1 retrolisthesis of T12 on L1 which appears degenerative. There is moderate-severe disc space narrowing on the right at L5-S1, with moderate narrowing on the left. The vertebral body and disk space heights are otherwise well-maintained. There is no acute fracture. At L4-L5, there is a right foraminal protrusion partially effaces the inferior foraminal fat with at most minimal foraminal narrowing. At L5-S1, there is a diffuse posterior osteophyte disc complex. If occult determine degree of central spinal canal narrowing secondary to the artifact. There does appear to be moderate right bony foraminal narrowing at this level. The left foramen is widely patent secondary to surgical changes. Limited views through the retroperitoneum demonstrate atherosclerotic calcification of the abdominal aorta. There is an oval cystic structure within the right aspect of the pelvis measuring 1.8 x 2.3 cm, likely a functional cyst associated with the right ovary. A small amount of free fluid is noted in the pelvis, likely physiologic if the patient remains premenopausal. There is a coarsely calcified structure in the uterus, likely relating to a fibroid. Hyperdense material is seen along the margins of a loop of colon.\n",
      "### Response:\n",
      "Transitional spinal anatomy as detailed above. If further surgery is to be contemplated, correlation with imaging of the entire spine is recommended. Expected postoperative changes following posterior surgical fusion of L5-S1. No acute fracture or instrumentation complication. Moderate bony right foraminal narrowing suggested at this level with widely patent left foramen. Central spinal canal not well assessed secondary to artifact. Right foraminal disc protrusion at L4-L5 with at most minimal right foraminal narrowing. Trace grade 1 degenerative retrolisthesis of T12 on L1.\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(val_data[254], 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b6571540-ee9f-4ccc-aced-b06eaa5a7fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "            ### Instruction:\n",
      "            Generate impression based on clinical information and findings.\n",
      "            ### Input:\n",
      "            Esophageal cancer status post chemo rads on chemo, creatine kinase response. Scattered pulmonary micronodules, some of which are calcified, indicating healed granulomatous disease. No suspicious pulmonary nodule or mass. No focal consolidation or pleural effusion. Again seen is bulky, heterogeneous, eccentric thickening of the distal esophagus, that appears to invade the gastric wall as well, and appears to have increased in size from the most recent prior as well as significantly when compared to the original study. This area measures approximately 33 mm in diameter, previously 26 mm, and before that 19 mm. Right chest port tip at the cavoatrial junction. The heart size is within normal limits, no significant pericardial effusion. Scattered subcentimeter mediastinal lymph nodes which are not enlarged by size criteria. Previously referenced subcarinal lymph node measures 9 mm in short axis, previously 10 mm. Additional calcified mediastinal and hilar lymph nodes related to old granulomatous process. Mild. No significant axillary lymphadenopathy. Scattered multilevel degenerative changes of the thoracolumbar spine. No suspicious osseous lesions are identified. Absence of enteric contrast material limits sensitivity for abdominal pathology. Stable hypodense right hepatic lesion measuring 18 x 18 mm, suggestive of a cyst. No intra or extrahepatic biliary dilatation. Gallbladder is normal. No significant abnormality noted. Subcentimeter right renal hypodensity is too small to characterize but likely represents a cyst. No hydronephrosis or nephrolithiasis. Again seen is a conglomerate of upper abdominal lymph nodes contiguous with the known esophageal mass currently measuring 3</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(val_data[1]['input_ids']).unsqueeze(0)\n",
    "generation_output = model.generate(\n",
    "        input_ids=input_ids.to('cuda'),\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "for s in generation_output.sequences:\n",
    "    print(tokenizer.decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166caa2-5fac-4415-bcd5-1f03e14b8355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "p39",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
